diff --git a/__pycache__/ca2c.cpython-310.pyc b/__pycache__/ca2c.cpython-310.pyc
index 9acff31..8684d08 100644
Binary files a/__pycache__/ca2c.cpython-310.pyc and b/__pycache__/ca2c.cpython-310.pyc differ
diff --git a/ca2c.py b/ca2c.py
index 6ff230d..06a0061 100644
--- a/ca2c.py
+++ b/ca2c.py
@@ -48,9 +48,7 @@ class CA2C(object):
                  gamma: float = 0.99,
                  initial_epsilon: float = 0.01,
                 final_epsilon: float = 0.01,
-                batch_size: int = 256,
                 learning_starts: int = 100,
-                buffer_size: int = int(1e6),
                 epsilon_decay_steps: int = None,  # None == fixed epsilon
                  log: bool = True,
                  seed: Optional[int] = None,
@@ -71,7 +69,6 @@ class CA2C(object):
         self.initial_epsilon = initial_epsilon
         self.epsilon = initial_epsilon
         self.final_epsilon = final_epsilon
-        self.batch_size = batch_size
         self.epsilon_decay_steps = epsilon_decay_steps
         self.learning_starts = learning_starts
         
@@ -86,12 +83,6 @@ class CA2C(object):
         self.info ={}
         
         
-        self.replay_buffer = self.ReplayBuffer(
-                self.state_dim,
-                1,
-                max_size=buffer_size,
-                action_dtype=np.uint8,
-            )
         
         self.log = log
         if log:
@@ -131,10 +122,15 @@ class CA2C(object):
             s_, r, missing, traveling, done, trcthu, info = env.step(a)  #return observation, reward, missing, traveling, terminated, truncated, info
             self.global_step += 1
             
-            self.add(s, a, r, s_, done)
-            
             if self.global_step >= self.learning_starts:
-                self.linearly_decaying_value(self.initial_epsilon,
+                if done :
+                    dw = True
+                else:
+                    dw = False
+
+                self.update(s, a, r, s_, dw)
+
+                self.epsilon = self.linearly_decaying_value(self.initial_epsilon,
                                                 self.epsilon_decay_steps,
                                                 self.global_step,
                                                 self.learning_starts,
@@ -217,135 +213,19 @@ class CA2C(object):
         )
 
 
-                
-    def ReplayBuffer(self, 
-        obs_shape,
-        action_dim,
-        max_size=100000,
-        obs_dtype=np.float32,
-        action_dtype=np.float32,
-    ):
-
-        """Initialize the replay buffer.
-
-        Args:
-            obs_shape: Shape of the observations
-            action_dim: Dimension of the actions
-            rew_dim: Dimension of the rewards
-            max_size: Maximum size of the buffer
-            obs_dtype: Data type of the observations
-            action_dtype: Data type of the actions
-        """
-        self.max_size = max_size
-        self.ptr, self.size = 0, 0
-        print(max_size,obs_shape)
-        print(type(max_size), type(obs_shape))
-        self.obs = np.zeros((max_size, obs_shape), dtype=obs_dtype)
-        self.next_obs = np.zeros((max_size,  obs_shape), dtype=obs_dtype)
-        self.actions = np.zeros((max_size, action_dim), dtype=action_dtype)
-        self.rewards = np.zeros((max_size, 1), dtype=np.float32)
-        self.dones = np.zeros((max_size, 1), dtype=np.float32)
-
-    def add(self, obs, action, reward, next_obs, done):
-        """Add a new experience to the buffer.
-
-        Args:
-            obs: Observation
-            action: Action
-            reward: Reward
-            next_obs: Next observation
-            done: Done
-        """
-        self.obs[self.ptr] = np.array(obs).copy()
-        self.next_obs[self.ptr] = np.array(next_obs).copy()
-        self.actions[self.ptr] = np.array(action).copy()
-        self.rewards[self.ptr] = np.array(reward).copy()
-        self.dones[self.ptr] = np.array(done).copy()
-        self.ptr = (self.ptr + 1) % self.max_size
-        self.size = min(self.size + 1, self.max_size)
+    
         
     def close_wandb(self) -> None:
         """Closes the wandb writer and finishes the run."""
 
         wandb.finish()
 
-    def sample(self, batch_size, replace=True, use_cer=False, to_tensor=False, device=None):
-        """Sample a batch of experiences from the buffer.
-
-        Args:
-            batch_size: Batch size
-            replace: Whether to sample with replacement
-            use_cer: Whether to use CER
-            to_tensor: Whether to convert the data to PyTorch tensors
-            device: Device to use
-
-        Returns:
-            A tuple of (observations, actions, rewards, next observations, dones)
-
-        """
-        inds = np.random.choice(self.size, batch_size, replace=replace)
-        if use_cer:
-            inds[0] = self.ptr - 1  # always use last experience
-        experience_tuples = (
-            self.obs[inds],
-            self.actions[inds],
-            self.rewards[inds],
-            self.next_obs[inds],
-            self.dones[inds],
-        )
-        if to_tensor:
-            return tuple(map(lambda x: th.tensor(x).to(device), experience_tuples))
-        else:
-            return experience_tuples
-
-    def sample_obs(self, batch_size, replace=True, to_tensor=False, device=None):
-        """Sample a batch of observations from the buffer.
-
-        Args:
-            batch_size: Batch size
-            replace: Whether to sample with replacement
-            to_tensor: Whether to convert the data to PyTorch tensors
-            device: Device to use
-
-        Returns:
-            A batch of observations
-        """
-        inds = np.random.choice(self.size, batch_size, replace=replace)
-        if to_tensor:
-            return th.tensor(self.obs[inds]).to(device)
-        else:
-            return self.obs[inds]
-
-    def get_all_data(self, max_samples=None):
-        """Get all the data in the buffer (with a maximum specified).
-
-        Args:
-            max_samples: Maximum number of samples to return
-
-        Returns:
-            A tuple of (observations, actions, rewards, next observations, dones)
-        """
-        if max_samples is not None:
-            inds = np.random.choice(self.size, min(max_samples, self.size), replace=False)
-        else:
-            inds = np.arange(self.size)
-        return (
-            self.obs[inds],
-            self.actions[inds],
-            self.rewards[inds],
-            self.next_obs[inds],
-            self.dones[inds],
-        )
-
-    def __len__(self):
-        """Get the size of the buffer."""
-        return self.size
 
             
 
 
 
-    def abc(self, s, a, r, s_, dw):
+    def update(self, s, a, r, s_, dw):
         s = torch.unsqueeze(torch.tensor(s, dtype=torch.float), 0)
         s_ = torch.unsqueeze(torch.tensor(s_, dtype=torch.float), 0)
         v_s = self.critic(s).flatten()  # v(s)
@@ -370,6 +250,16 @@ class CA2C(object):
         self.I *= self.GAMMA  # Represent the gamma^t in th policy gradient theorem
         
         
+        if self.log and self.global_step % 100 == 0:
+            wandb.log(
+                    {
+                        "losses/critic_loss": critic_loss,
+                        "losses/epsilon": self.epsilon,
+                        "losses/actor_loss": actor_loss,
+                        "global_step": self.global_step,
+                    },)
+            
+        
     def register_additional_config(self, conf: Dict = {}) -> None:
         """Registers additional config parameters to wandb. For example when calling train().
 
@@ -386,7 +276,6 @@ class CA2C(object):
             "initial_epsilon": self.initial_epsilon,
              "final_epsilon": self.final_epsilon,
             "epsilon_decay_steps:": self.epsilon_decay_steps,
-            "batch_size": self.batch_size,
             "gamma": self.GAMMA,
             "learning_starts": self.learning_starts,
             "seed": self.seed,
diff --git a/policy_constraint.py b/policy_constraint.py
index 2f6a662..28fa1ac 100644
--- a/policy_constraint.py
+++ b/policy_constraint.py
@@ -57,11 +57,9 @@ def main(args):
                  initial_epsilon=1.0,
                  final_epsilon=0.01,
                  seed=984,
-                epsilon_decay_steps=10000,
-                batch_size=8,
-                learning_starts=1000,
-                buffer_size=int(1e5),
-                 log=False,
+                epsilon_decay_steps=2000,
+                learning_starts=100,
+                 log=True,
                  project_name="skip",
                 experiment_name="RCPO",)
 
